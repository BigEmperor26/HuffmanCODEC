\section{Performance and benchmarking}
In this section we are going to discuss the performance evaluation of our algorithm. We start from evaluating some other implementations of Huffman algorithm and then focus on the analysis of our own implementation. 

We include in this section some graphs and statistics, but all detailed results are included in the appendix at the end this report.
\subsection{Setup}
All the tests have been performed on the HPC2 cluster of the University of Trento. To reduce as much as possible the I/O timings, which are crucial of our application, jobs were run in a very specific setting:

\begin{itemize}
	\item Threads are allocated in the same node and, if possible, on the same socket to guarantee fast communication between them. This has been achieved by using the MPI option \verb|--map-by|.
	\item \textcolor{red}{Processes are allocated in different nodes, by using the PBS directive} \verb|place=scatter:excl|. This because in our application different processes do not communicate very often between each other, but they just split their work and use the file-system. 
\end{itemize}

The provided timings are obtained by averaging the result of three runs with the exact same configurations: this helps to minimize the effect random variance between runs due to potential hardware congestion.
Each job has been submitted to the cluster individually, waiting for the previous to finish preventing hiccups due to multiple instances of the program trying to access the same file-system resource, which we noticed to greatly affect I/O times.

We extensively verified the correctness of our algorithm by encoding and then decoding a file, then comparing the decoded result to the original file by using the \verb|diff| command. Moreover, Valgrind has been used to spot any memory leaks that could cause runtime errors.

\subsection{Datasets}
The evaluation dataset has been randomly generated by using a simple C program: we created text files with character frequencies that follow the occurrences of the letters in English language. Although for benchmarking we are only using ASCII characters, please note that our algorithm reads bytes of data and can therefore work with any file.
The dataset is composed by files of increasing size: 1 MiB, 5 MiB, 10 MiB, 50 MiB, 100 MiB, 500 MiB, 1 GiB, 5 GiB, and 10 GiB. This should be enough range to understand how different algorithms scale with increasing file sizes.

Secondly, because our algorithm also works with many files at once, we chose some GitHub repository as benchmark: Numpy, PyTorch and Linux kernel as benchmarks. They are respectively 30MiB, 200MiB and  1.3 GiB of total size, with about $\sim$ 2.000, $\sim$ 12.000, and  $\sim$ 80.000 files, which on average are each 16MiB in size.

Considering the dataset sizes and amount testing we have done, we estimate to have accessed the disk (both read and writes) for about $\sim$ 6 TiB of data for our whole benchmark analysis.

\subsection{Other works}

\subsection{Multithreading evaluation}
In this section we evaluate our algorithm on single files, by using multithreading and testing our parallelization performances with an increasing number of threads.

With small files (i.e. less than a few MiBs) the algorithm is actually slower with more threads. This is likely to be the case because the data is not large enough to offset the cost of creating multiple threads. Considering the cost parallel processing, we highly suggest not doing parallelization for small files.

However, when the data is large enough our multithreaded approach scales up. With the largest file (10 GiB) our algorithm has an efficiency of 93\% when tested with 4 threads. This number steadily decreases with the number of threads, and we hit 32\% with 24 threads.

Please note that our tool is heavily limited by the cluster I/O, since we can not make it parallel: especially writings to disk require a considerable amount of time. As a proof of this, if we consider only the processing section of our algorithm, and ignore the time required by the \verb|fflush()| operation before a \verb|fclose()|, we see significantly decreased times. Although we start from similar efficiency at low threading, we achieve a 54\% efficiency with 24 threads in this case: if we consider only CPU time, which does not count the time spent waiting in I/O, we get very promising results.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"../imgs/Flush vs non Flush"}
	\caption{Encoding times with and without the flush()}
	\label{fig:flush-vs-non-flush}
\end{figure}

We also noticed worse times in the decoding procedure with respect to encoding. This is probably due to the fact that the tool is reading fixed chunks of 4096 bytes in encoding, but during decoding the chunk can have a variable length.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"../imgs/Encoding vs Decoding"}
	\caption{Encoding vs Decoding times}
	\label{fig:encoding-vs-decoding}
\end{figure}

As already discussed previously, we also tested a version on a lock-based synchronization instead of barriers. Although theoretically we should get better results, in practice we obtain almost equal performances across the board, with only the largest files having some significant difference. This difference is greater in decoding than in encoding.
\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"../imgs/Barrier vs Locks encoding"}
	\caption{Barrier vs Lock based synchronization performance}
	\label{fig:barrier-vs-locks-encoding}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"../imgs/Encoding Barrier times"}
	\caption{Encoding times}
	\label{fig:encoding-barrier-times}
\end{figure}

\subsection{Multiprocessing}
We tested on different folders as benchmark the parallelization performances of our algorithm. Theoretically, our algorithm should scale very well with increasing number of processes and files, because once the main process sends the list of files to encode to each process there is no further communication between different processes. However, we find that real world results are different. 
While we don't expect much increasing in the performance by the number of threads, because the files are on average very small, we should get better results by using many processes.
In reality our testing shows that this is not the case. Our efficiencies are extremely low. We believe the main limitation of our algorithm is the I/O of the cluster. When tested on a local machine (MacBook A2442) with one thread and 1,2,4 processes, we find that our algorithm does indeed scale with the number of processes.

\subsection{Memory, storage and other considerations}
Our memory requirements don't scale upwards with the size of the files we need to encode, because at any given time, the most we only store is the size of two buffers, each is a \verb|unsigned char buffer[num_threads][4096]|. These buffers are emptied at each I/O cycle, therefore leading to a high memory efficiency. Only when processing multiple files our memory requirements scale up with the number of files, as each process needs to store information about the files is assigned to process.

We also find that we achieve on average 48\% compression rate of our synthetic data. Instead, on Linux kernel, because contains more varied data which consists of mostly code, we achieve a lower 62\% compression rate. Testing on other already compressed data as \verb|.zip| or \verb|AV1| resulted in a compression rate of 100\%.
