\section{Performance and benchmarking}
In this section we are going to discuss the performance evaluation of our algorithm. We start from evaluating some other implementations of Huffman algorithm and then focus on the analysis of our own implementation. 

We include in this section some graphs and statistics, but all detailed results are included in the appendix at the end this report.
\subsection{Setup}
All the tests were performed on the HPC2 cluster of the University of Trento. To reduce as much as possible the I/O timings, which are crucial of our application, jobs were run in a very specific setting:

\begin{itemize}
	\item Threads are allocated in the same node and, if possible, on the same socket to guarantee fast communication between them. This has been achieved by using the MPI option \verb|--map-by|.
	\item Processes are allocated in different nodes, by using the PBS directive \verb|place=scatter:excl|. This because in our application different processes do not communicate very often between each other, but they just split their work and use the file-system. We also tested \verb|place=pack:excl| but found worse results.
\end{itemize}

The provided timings are obtained by averaging the result of three runs with the exact same configurations: this helps to minimize the effect random variance between runs due to potential hardware congestion.
Each job has been submitted to the cluster individually, waiting for the previous to finish preventing hiccups due to multiple instances of the program trying to access the same file-system resource, which we noticed to greatly affect I/O times.

We extensively verified the correctness of our algorithm by encoding and then decoding a file, then comparing the decoded result to the original file by using the \verb|diff| command. Moreover, Valgrind has been used to spot any memory leaks that could cause runtime errors.

\subsection{Datasets}
The evaluation dataset has been randomly generated by using a simple C program: we created text files with character frequencies that follow the occurrences of the letters in English language. Although for benchmarking we are only using ASCII characters, please note that our algorithm reads bytes of data and can therefore work with any file.
The dataset is composed by files of increasing size: 1 MiB, 5 MiB, 10 MiB, 50 MiB, 100 MiB, 500 MiB, 1 GiB, 5 GiB, and 10 GiB. This should be enough range to understand how different algorithms scale with increasing file sizes.

Secondly, because our algorithm also works with many files at once, we chose some GitHub repositories as benchmark: Numpy\cite{2020NumPy-Array}, PyTorch \cite{Paszke_PyTorch_An_Imperative_2019} and Linux kernel \cite{linux}. They are respectively $\sim$ 30 MiB,$\sim$ 200 MiB and $\sim$ 1.3 GiB of total size, with about $\sim$ 2.000, $\sim$ 12.000, and  $\sim$ 80.000 files, which on average are each 16MiB.

% Considering the dataset sizes and amount testing we have done, we estimate to have accessed the disk (both read and writes) for about $\sim$ 6 TiB of data for our whole benchmark analysis.


\subsection{Multithreading evaluation}
In this section we evaluate our algorithm on single files, by using multithreading and testing our parallelization performances with an increasing number of threads.

With small files (i.e. less than a few MiBs) the algorithm is actually slower with more threads. This is likely because the data is not large enough to offset the cost of forking multiple threads. Considering the cost parallel processing, we highly suggest not doing parallelization for small files. However, when the data is large enough our multithreaded approach scales up. With the largest file (10 GiB) our algorithm has an efficiency of 93\% when tested with 4 threads. This number steadily decreases with the number of threads, and we hit 16\% with 64 threads.

The same issue is even more visible by looking at one implementation we found on GitHub that uses only MPI for parallelization\cite{HuffmanCodingMPICUDA}: with many processes, while with large files (even though it does not work with files bigger than $\sim$500 MiB) it performs better than our implementation, with small files it is not able to scale. This is happening due to the overhead of splitting small files among too many processes.

Additionally, please note that our tool is heavily limited by the cluster I/O: especially writes to disk require a considerable amount of time. As a proof of this, if we consider only the processing section of our algorithm, and ignore the time required by the \verb|fflush()| operation before \verb|fclose()|, we see significantly decreased times. Although we start from similar efficiency at low threading, we achieve a 41\% efficiency with 64 threads in this case. % if we consider only CPU time, which does not count the time spent waiting in I/O, we get very promising results.

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../imgs/Flush vs non Flush"}
	\caption{Encoding times with and without the flush()}
	\label{fig:flush-vs-non-flush}
\end{figure}

We also noticed worse times in the decoding procedure with respect to encoding. This is probably due to the fact that the tool is reading fixed chunks of 4096 bytes in encoding, but during decoding the chunk can have a variable length.
Moreover, by executing another implementation found on GitHub \cite{HuffmanParallel2}, we found that reading and writing buffers to the disk (and not single bytes) can obtain improvement very similar to one order of magnitude.

% \begin{figure}
% 	\centering
% 	\includegraphics[width=1\linewidth]{"../imgs/Encoding vs Decoding"}
% 	\caption{Encoding vs Decoding times}
% 	\label{fig:encoding-vs-decoding}
% \end{figure}

As already discussed previously, we also tested a version on a lock-based synchronization instead of barriers. Although theoretically we should get better results, in practice we obtain almost equal performances across the board, with only the largest files having some significant difference. This difference is greater in decoding than in encoding.
% \begin{figure}
% 	\centering
% 	\includegraphics[width=1\linewidth]{"../imgs/Barrier vs Locks encoding"}
% 	\caption{Barrier vs Lock based synchronization performance}
% 	\label{fig:barrier-vs-locks-encoding}
% \end{figure}

\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../imgs/encoding speedup wide"}
	\caption{Encoding speedup}
	\label{fig:encoding-speedup}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../imgs/encode efficiency wide"}
	\caption{Encoding efficiency}
	\label{fig:encoding-efficiency}
\end{figure}


\subsection{Multiprocessing}
We tested on different folders the parallelization performances of our algorithm. Theoretically, it should scale very well with increasing number of processes and files, because once the main process sends the list of files to encode to each process there is no further communication between different processes. However, we find that real world results are different.

While we do not expect much performance improvement by the number of threads, because the files are on average very small, we should get better results by using many processes. In reality, our testing shows that this is not the case, probably due to the slow cluster I/O. When tested on a local machine (MacBook A2442) with one thread and 1,2,4 processes, we find that our algorithm does indeed scale with the number of processes.
We also find that \verb|scatter| scales better than \verb|pack|, probably because with \verb|scatter| we are not limited by the I/O of a single node.
\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{"../imgs/linux speedup"}
	\caption{Linux speedup for different placing strategies}
	\label{fig:linux-scatter-pack}
\end{figure}

% \subsection{Memory, storage and other considerations}
% We also find that we achieve on average 48\% compression rate of our synthetic data. Instead, on Linux kernel, because contains more varied data which consists of mostly code, we achieve a lower 62\% compression rate. Testing on other already compressed data as \verb|.zip| or \verb|AV1| resulted in a compression rate of 100\%.
