\section{Performance and benchmarking}
Performance evaluation.

Here we discuss the perfomance evaluation of our algorithm


All data is resulting average of 3 runs.
We tested on HPC2 of unitn cluster.
We tested everything on a single  node. We used the map by command to map the correct number of cores to each MPI process.
We always requested 4GiB of memory, but in reality our program needs far less.
We don't use much memory because at any given time, the most we only store is the size of two buffers, each is a \verb|unsigned char buffer[m][buffer_size]| and then they are emptied at each cycle.
Each datapoint was submitted as a job to the cluster, individually. No other jobs were assigned to the cluster and and no other I/O operations was done. This is to prevent hiccups due to multiple instances of the program trying to access the same data.

Dataset were pragmatically generated using a C program, creating text files with characters that follow the frequency of letters in English language.

We read and wrote about $\sim$ 6 TiB of data for our whole benchmark analysis.

We can see that our algorithm scales very well. 

With small files, less than a few MiBs, the algorithm is actually slower with more threads. This is likely because the data is not large enough. The parallel procesng is not enough to offset the cost of parallezation using threads.

However when the data is large, we can see that our approeach does actually perfom well. With the largest file ( 10 GiB ) our algorithm has an efficiency of 93\% when tested with 4 threads. This number steadily decreases with the number of threads, until a measily 32\% with 24 threads. 

It has to be noted that we are heavily limited by the I/O. We are unable to parallelize the I/O, resulting in long waiting times, especially the writing operations. As a proof of this, if we consider only the processing section of our algorithm, and ignore the time required by the flush() operation before a fclose(), we see significantly better times. Same from 92\% at 4 threads, but we scale much better at 55\% with 24 threads. 

Some images

Other stuff

Folders.

We also tested the ability to scale with processors. We chose as benchmark Linux kernel ( available here ). It is about 266 MiB of total size, with about 80.000 files.


We also tested on


The correctness of our algorithm was easily verified by encoding and then decoding a file, then comparing it to the original with the diff command.